{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and Information Retrieval with Julia\n",
    "______________________________________________________________\n",
    "\n",
    "![pipes](images/text_classification_workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The data source is usually a document database such as MongoDB. I've started a client with a local mongo service and loaded the data from a json document that is supposed to mimic the real-life document one might recieve from a request body. I won't go into how to do this with this tutorial so lets assume the documents are already loaded in a local Mongo service.\n",
    "\n",
    "\n",
    "\n",
    "### Loading Data from Mongo\n",
    "\n",
    "For my pipeline, I want the data in a julia table. I want to pipe and process the data from my database to flat files (txt) located in a directory called data. To do this I will call a python script, aptly called load_data.py. I did this in python as I already had pymongo installed and the main goal is to perform common nlp tasks with julia. Number of files written will be 0 if files already exist locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files written:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Process(`\u001b[4mpython\u001b[24m \u001b[4msrc/load_data.py\u001b[24m`, ProcessExited(0))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(`python src/load_data.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Pipeline\n",
    "\n",
    "The goal is to build a basic text processing pipeline involving tokenization, stripping stopwords and stemming. Ultimately what we want is a sparse representation of the data where 1 row of data is a document and each column is a unique term, such as a unigram, bigram or trigram. The values herein will be generated from a vectorization method which assigns each document term a value which is proportional to its frequency in the document, but inversely proportional to the number of documents in which it occurs.\n",
    "\n",
    "### Load data using TextAnalysis and Glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TextAnalysis\n",
    "using Glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array of filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob(\"data/*.txt\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read an example file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42-element Array{String,1}:\n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"Hey, the man on the phone said. Are you still coming tonight?        \"                                                         \n",
       " \" \"                                                                                                                             \n",
       " â‹®                                                                                                                               \n",
       " \"Recommended B.L.T.; chilled charred broccoli; porgy; burger; wings; country fried duck and waffles; pork ribs; smores.        \"\n",
       " \"Prices \\$5 to \\$29.        \"                                                                                                   \n",
       " \"Open Nightly for dinner, Saturday and Sunday for brunch.        \"                                                              \n",
       " \"Reservations Accepted.        \"                                                                                                \n",
       " \"Wheelchair access Entrance is up a short flight of stairs from the sidewalk. Restrooms have handrails.        \"                \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \" \"                                                                                                                             \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              \n",
       " \"\"                                                                                                                              "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example file\n",
    "readlines(fnames[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also map a file document type to the filenames in fnames. This produces an array of FileDocuments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = map(FileDocument, fnames);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read a file using the text function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHey, the man on the phone said. Are you still coming tonight?        \\n \\n\\nIt took a moment for me to realize that he was calling from Distilled to confirm my dinner reservation.        \\nYes, I replied. Cool, he said, and sounded as if he meant it.        \\nDistilled opened in June on the corner of Franklin Street and West Broadway in TriBeCa, the former home of Drew Nieporents Layla and Centrico. The belly dancers and the frozen-margarita machine are gone, but a certain effervescence remains. So does Mr. Nieporent, hovering in the background as guru to Distilleds owners, the first-time restaurateur Nick Iovacchini and Shane Lyons, the 25-year-old chef.        \\nThe space is blandly handsome, with dark woods and charcoal banquettes, breathlessly high ceilings and quasi-medieval wheel chandeliers like crowns of fire. One side is devoted to the bar, where the drinks, by Benjamin Wood, are lady-killers, elegant with a knife twist. Occasionally 1980s mope rock shimmers from the speakers.        \\nService is confoundingly friendly, almost coddling. When I stood outside reading the posted menu, someone came hurrying down the steps to hand me my own copy, so I wouldnt crane my neck, he said. On arrival and departure, a host leapt to open the door.        \\nThe mission statement that preceded one meal (We are a modern American public house, the waiter intoned) was both unnecessary and slightly coy about Mr. Lyonss ambitions. Yes, wings are on the menu, but they are jacked up with gochujang, Korean fermented soybean and chile paste  borrowed, perhaps, from the larder at Momofuku Noodle Bar, where Mr. Lyons worked for a year.        \\nThere are occasional technical flourishes, like watermelon cubes Cryovaced to intensify their flavor, and mushrooms surrounded by puffs of buttery onion soubise, aerated by an iSi siphon. A tousle of dehydrated and shaved bacon adorns an open-face sandwich of heirloom tomatoes and basil on sourdough: a B.L.T., of course. Sunflower sprouts make up for the missing crunch.        \\nOther classics are updated rather than upended: popcorn dusted with garlic, cumin and brewers yeast, which evokes cheese; snappy pickles fermented with gochugaru, Korean red chile powder; pork ribs glazed with more gochujang, teasing the sweet-salty border without straying too far in either direction.        \\nOnion rings, battered with Yuengling beer and tapioca flour, are fried, then frozen (a theatrical waiter boasted that they had been brought to negative 60 degrees) and fried again. They arrive nicely sturdy, the sole purpose of their existence to ferry the narcotic-like condiments, burned scallions cut with jalapeos and mayonnaise with the sting of preserved lime.        \\nThe substantial burger (which the menu modestly refrains from telling you is made with grass-fed, organic-grain-finished beef) is abetted by what may be the finest version of Tater Tots in town, the grated potatoes cooked until just underdone and then crisped with Wondra flour.        \\nEverything here goes to 11, one diner marveled. Duck breaded and fried like chicken is brazen and irresistible, despite the oversweet accompanying waffle, a spongy slab of brioche dredged in custard, like French toast. Even broccoli turns wanton, flung with slivers of duck bacon and pickled watermelon rind in a fish sauce vinaigrette, with (wait for it) a dollop of duck fat.        \\nBut liver pt served with plumes of baked, dehydrated chicken skin? Now this is confrontational. My mission in life is to make skinny girls fat, Mr. Lyons told my table. He got a laugh, but the cracklings were abandoned after one bite.        \\nThe only logical end to such a meal is smores  deconstructed, as is the fashion, with a hickory-smoked graham-flour cake, a torched smear of marshmallow fluff, dark chocolate pudding and graham crackers broken on top. It is obvious and no less pleasurable for it. (Kari Rak, previously at Bouchon Bakery, will introduce a new dessert menu this month.)        \\nOr take a shot of moonshine, with an apricot shrub as a chaser. It edges everything in halos and can make you believe that a modern American public house, whatever that is, is where you want to be.        \\nDistilled \\n211 West Broadway (Franklin Street); (212) 601-9514; distilledny.com.        \\nRecommended B.L.T.; chilled charred broccoli; porgy; burger; wings; country fried duck and waffles; pork ribs; smores.        \\nPrices \\$5 to \\$29.        \\nOpen Nightly for dinner, Saturday and Sunday for brunch.        \\nReservations Accepted.        \\nWheelchair access Entrance is up a short flight of stairs from the sidewalk. Restrooms have handrails.        \\n\\n\\n\\n \\n\\n\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text(fds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do this would be to use the core Julia functions to load text. We can push strings into an iterable data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "slist = String[]\n",
    "for fname in fnames\n",
    "    s = open(fname) do file\n",
    "        # read the contents of a file all at once\n",
    "        read(file, String)\n",
    "    end\n",
    "    push!(slist, s)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metadata for our document can be accesed as a property of the FileDocument instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextAnalysis.DocumentMetadata(Languages.English(), \"data/5233240838f0d8062fddf624.txt\", \"Unknown Author\", \"Unknown Time\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = fds[1];\n",
    "a.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Stop Words\n",
    "\n",
    "Next we will be removing stop words and tokenizing the document. Tokens are individual words split on whitespace. Stop words are high frequency words that we want to filter out. These words often have low lexical meaning and they don't help distinguish one text from another. Below I've created my_prepare which takes care of preparation tasks such as stripping punctuation, articles, pronouns, numbers and non-letters. This also removes stopwords. and stems the document which removes morphological affixes to the words leaving only the stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "using WordTokenizers\n",
    "using Languages\n",
    "\n",
    "set_tokenizer(WordTokenizers.nltk_word_tokenize)\n",
    "\n",
    "STOPWORDS = stopwords(Languages.English());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_prepare"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "my_prepare(text)\n",
    "\n",
    "Returns prepared text string\n",
    "\"\"\"\n",
    "function my_prepare(text)\n",
    "    sd = StringDocument(text)\n",
    "    prepare!(sd, strip_punctuation\n",
    "        | strip_articles \n",
    "        | strip_pronouns\n",
    "        | strip_numbers \n",
    "        | strip_non_letters)\n",
    "    remove_words!(sd, STOPWORDS)\n",
    "    stem!(sd)\n",
    "    remove_case!(sd)\n",
    "    return sd.text\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hey phone are come tonight it moment realiz call distil confirm dinner reserv yes repli cool sound meant distil june corner franklin street west broadway tribeca former home drew niepor layla centrico the belli dancer frozen margarita machin gone effervesc remain so mr niepor hover background guru distil owner time restaurateur nick iovacchini shane lyon chef the space bland handsom dark wood charcoal banquett breathless ceil quasi mediev wheel chandeli crown fire one devot bar drink benjamin wood ladi killer eleg knife twist occasion mope rock shimmer speaker servic confound friend coddl when stood outsid read post menu hurri step hand copi wouldnt crane neck on arriv departur host leapt door the mission statement preced meal we modern american public hous waiter inton unnecessari slight coy mr lyonss ambit yes wing menu jack gochujang korean ferment soybean chile past borrow larder momofuku noodl bar mr lyon there occasion technic flourish watermelon cube cryovac intensifi flavor mushroom surround puff butteri onion soubis aerat isi siphon a tousl dehydr shave bacon adorn sandwich heirloom tomato basil sourdough b l t cours sunflow sprout miss crunch other classic updat upend popcorn dust garlic cumin brewer yeast evok chees snappi pickl ferment gochugaru korean red chile powder pork rib glaze gochujang teas sweet salti border stray direct onion ring batter yuengl beer tapioca flour fri frozen theatric waiter boast brought negat degre fri they arriv nice sturdi sole purpos exist ferri narcot condiment burn scallion cut jalapeo mayonnais sting preserv lime the substanti burger menu modest refrain tell grass fed organ grain finish beef abet finest version tater tot town grate potato cook underdon crisp wondra flour everyth goe diner marvel duck bread fri chicken brazen irresist despit oversweet accompani waffl spongi slab brioch dredg custard french toast even broccoli wanton flung sliver duck bacon pickl watermelon rind fish sauc vinaigrett wait dollop duck fat but liver pt serv plume bake dehydr chicken skin now confront my mission life skinni girl fat mr lyon told tabl he laugh crackl abandon bite the logic meal smore deconstruct fashion hickori smoke graham flour cake torch smear marshmallow fluff dark chocol pud graham cracker broken top it obvious pleasur kari rak previous bouchon bakeri introduc dessert menu month or shot moonshin apricot shrub chaser it edg halo believ modern american public hous whatev distil west broadway franklin street distilledni com recommend b l t chill char broccoli porgi burger wing countri fri duck waffl pork rib smore price open night dinner saturday sunday brunch reserv accept wheelchair access entranc short flight stair sidewalk restroom handrail\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prepare(text(fds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words and TFIDF\n",
    "\n",
    "Using the TextAnalysis package we will create a DirectoryCorpus to use when constructing counts over the whole corpus. A text corpus is a large body of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps = DirectoryCorpus(\"data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the standardize inplace function to make sure all the documents in our corpus are standardized to the StringDocument type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize!(crps, StringDocument)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use some of the preparation steps from my_preparation function above but applied to the entire corpus. These work in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_case!(crps)\n",
    "prepare!(crps, strip_punctuation\n",
    "    | strip_articles \n",
    "    | strip_pronouns\n",
    "    | strip_numbers \n",
    "    | strip_non_letters)\n",
    "remove_words!(crps, STOPWORDS)\n",
    "stem!(crps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our lexicon is what is going to keep track of our words and the counts associated with each word. The is in the form of a dictionary. First we have to update the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_lexicon!(crps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Int64} with 22718 entries:\n",
       "  \"nuhu\"        => 1\n",
       "  \"ironwe\"      => 1\n",
       "  \"wintri\"      => 2\n",
       "  \"flatb\"       => 1\n",
       "  \"economix\"    => 2\n",
       "  \"curv\"        => 21\n",
       "  \"skylight\"    => 4\n",
       "  \"unoffici\"    => 5\n",
       "  \"touchpad\"    => 1\n",
       "  \"bidder\"      => 4\n",
       "  \"whiz\"        => 3\n",
       "  \"beckett\"     => 5\n",
       "  \"brandt\"      => 5\n",
       "  \"apiec\"       => 4\n",
       "  \"il\"          => 3\n",
       "  \"msnbc\"       => 3\n",
       "  \"archiv\"      => 25\n",
       "  \"overdos\"     => 2\n",
       "  \"ankl\"        => 26\n",
       "  \"oedip\"       => 1\n",
       "  \"adventur\"    => 25\n",
       "  \"acton\"       => 1\n",
       "  \"wpp\"         => 8\n",
       "  \"recurr\"      => 2\n",
       "  \"underground\" => 19\n",
       "  â‹®             => â‹®"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon(crps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to have a reverse lookup for each word with documents it appears in, we need to create an inverse index. Fortunately, TextAnalysis makes this easy for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_inverse_index!(crps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Array{Int64,1}} with 22718 entries:\n",
       "  \"nuhu\"        => [603]\n",
       "  \"ironwe\"      => [630]\n",
       "  \"wintri\"      => [159, 583]\n",
       "  \"flatb\"       => [744]\n",
       "  \"economix\"    => [611, 809]\n",
       "  \"curv\"        => [23, 47, 51, 272, 284, 422, 493, 522, 559, 575, 584, 599, 61â€¦\n",
       "  \"skylight\"    => [360, 376, 530, 634]\n",
       "  \"unoffici\"    => [24, 123, 281, 719, 999]\n",
       "  \"touchpad\"    => [757]\n",
       "  \"bidder\"      => [104, 235, 881]\n",
       "  \"whiz\"        => [51, 321, 857]\n",
       "  \"beckett\"     => [397, 601, 701, 750, 992]\n",
       "  \"brandt\"      => [91, 493, 706, 800, 916]\n",
       "  \"apiec\"       => [328, 773, 816, 869]\n",
       "  \"il\"          => [234, 833, 939]\n",
       "  \"msnbc\"       => [89, 107, 746]\n",
       "  \"archiv\"      => [203, 243, 245, 368, 549, 560, 599, 676, 716, 826, 833, 878,â€¦\n",
       "  \"overdos\"     => [639, 758]\n",
       "  \"ankl\"        => [147, 158, 168, 241, 266, 375, 393, 408, 447, 462, 572, 662,â€¦\n",
       "  \"oedip\"       => [2]\n",
       "  \"adventur\"    => [2, 41, 174, 198, 211, 233, 238, 341, 386, 405  â€¦  551, 632,â€¦\n",
       "  \"acton\"       => [65]\n",
       "  \"wpp\"         => [87]\n",
       "  \"recurr\"      => [431, 763]\n",
       "  \"underground\" => [32, 168, 227, 427, 557, 630, 631, 634, 647, 679, 723, 769, â€¦\n",
       "  â‹®             => â‹®"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_index(crps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DocumentTermMatrix(crps);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DocumentTermMatrix is a struct with properties containing the components necessary to create a term frequency inverse document frequency (tfidf) matrix for the corpus. This will be applied in later procedures involving information retrieval or sentiment analysis.\n",
    "\n",
    "The document term matrix is stored in a data structure called SparseMatrixCSC. Sparse matrices are distinct from dense matrices in that the only values stores are non-zero values. In julia, zero values can be stored but only manually. Sparse matrices are common in machine learning, such as in data that contains counts and data encodings that map values to n dimensional arrays, because these computationally efficient data structures can elicit performance gains when used by algorithms meant to take advantage of sparsity.\n",
    "\n",
    "The inverse index also provides us with a count of the number of documents each word appears in. This is known as document frequencies.\n",
    "\n",
    "To obtain the tfidf matrix we will simply use the function below applied to the document term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf_idf(m);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for computing tfidf\n",
    "\n",
    "What if we want to do all of the above manually?\n",
    "\n",
    "1. Create the bag of words (bow), a set of words unique over the corpus. A set is a good datatype for this since it doesn't allow duplicates. At the end you'll want to convert it to a list so that we can deal with our words in a consistent order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_docs = []\n",
    "bow = Set{String}();\n",
    "for doc in fds\n",
    "    cleaned = tokenize(my_prepare((text(doc))))\n",
    "    union!(bow, Set(cleaned))\n",
    "    push!(cleaned_docs, cleaned)\n",
    "end\n",
    "filter!(!isempty, cleaned_docs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a reverse lookup for the vocab list. This is a dictionary whose keys are the words and values are the indices of the words (the word id). This will make things much faster than using the list index function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = Dict{String,Int64}()\n",
    "for (i, word) in enumerate(bow)\n",
    "    indexer[word] = i\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a word count matrix. This is an array data type where each row corresponds to a document and each column a word. The value should be the count of the number of times that word appeared in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = length(fds)\n",
    "num_words = length(indexer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = zeros((num_docs, num_words));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (idx, doc) in enumerate(cleaned_docs)\n",
    "    C = Dict{String,Int64}()\n",
    "    for word in doc\n",
    "        C[word] = get(C, word, 0) + 1\n",
    "    end\n",
    "    for (word, count) in C\n",
    "        counts[idx, indexer[word]] = count\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create the document frequencies. For each word, get a count of the number of documents the word appears in. This is different from the total number of times the word appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sum(counts, dims=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Normalize the word count matrix to get the term frequencies. This means dividing each term frequency by the l2 (euclidean) norm. This makes each document vector has a length of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_frequencies\n",
    "tf_norm = sqrt.(sum(counts .^ 2, dims=2));\n",
    "tf_norm[tf_norm .== 0] .= 1;\n",
    "tf = counts ./ tf_norm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Multiply the term frequency matrix by the log of the inverse of the document frequencies to get the tf-idf matrix. We add one to the denominator to avoid dividing by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = log.((num_docs + 1) ./ (1 .+ df)) .+ 1;\n",
    "tfidf_m = tf .* idf;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Normalize the tf-idf matrix as well by dividing by the l2 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_norm = sqrt.(sum(tfidf_m .^ 2, dims=2));\n",
    "tfidf_norm[tfidf_norm .== 0] .= 1;\n",
    "tfidf_m ./= tfidf_norm;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
